{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60eb4d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import jieba\n",
    "import pickle\n",
    "import os\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6baf1cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load Data\n",
    "with open('../data/raw/z_channel_apartment.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "full_text = \" \".join([d['text'] for d in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33c19a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\MUZAFA~1.AZM\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.509 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "# 2. Load Custom Malaysian Dictionary\n",
    "# Ensure you have 'assets/msia_dict.txt' with terms like RTS and CIQ\n",
    "if os.path.exists('../assets/msia_dict.txt'):\n",
    "    jieba.load_userdict('../assets/msia_dict.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9f067bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Tokenize & Filter\n",
    "# Remove stop words and single-character words (grammar particles)\n",
    "stop_words = {'這個', '這邊', '就是', '一個', '可以', '他們', '因為', '現在', '如果', '一些', '什麼', '所以'}\n",
    "tokens = [t for t in jieba.lcut(full_text) if len(t) > 1 and t not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2f224b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Save for PCA (Notebook 03)\n",
    "os.makedirs('../data/processed', exist_ok=True)\n",
    "with open('../data/processed/clean_tokens.pkl', 'wb') as f:\n",
    "    pickle.dump(tokens, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "320ff5dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Phrases:\n",
      "      Word  Count\n",
      "0      RTS     16\n",
      "1      CIQ     16\n",
      "2  project     13\n",
      "3       連橋      9\n",
      "4       展商      9\n",
      "5    Block      9\n",
      "6       地段      8\n",
      "7       靠近      8\n",
      "8       真的      7\n",
      "9       一直      7\n"
     ]
    }
   ],
   "source": [
    "# 5. EDA: Top 10 Phrases\n",
    "print(\"Top 10 Phrases:\")\n",
    "print(pd.DataFrame(Counter(tokens).most_common(10), columns=['Word', 'Count']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca444f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
